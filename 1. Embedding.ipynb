{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c529ccd-5e7e-40e9-bcf4-62a9389093f4",
   "metadata": {},
   "source": [
    "# Retreival - Core of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44ff2a-3ca1-4378-8914-62e1807f118a",
   "metadata": {},
   "source": [
    "<img src='./images/IMG_0359.jpg' width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb24e6-7a34-45b7-a570-a64d32d31c13",
   "metadata": {},
   "source": [
    "## Context Tokenizer\n",
    "A context tokenizer is responsible for preprocessing raw text data by converting it into a sequence of tokens that a model can understand. Tokens are smaller units of text, such as words, subwords, or characters, depending on the tokenization strategy used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8113ab28-3567-4824-b66f-e9552938aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DPRContextEncoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d166014-b9ff-46c8-8e35-5dfa447cd19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739a42c2-8ca3-41ef-8222-d37c4df163cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [(\"How are you?\",\"I am Good.\"),(\"Hey Buddy. Whats Up?\",\"Nothing Man.\"),(\"What Is Love\",\"It is a song recorded by the artist Haddaway\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f33e8f-4a4e-47a4-9e26-1f674bdafef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_info = context_tokenizer(text , return_tensors='pt', padding=True, truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3a88628-3738-4f92-9e21-276cff25f48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2129, 2024, 2017, 1029,  102, 1045, 2572, 2204, 1012,  102,    0,\n",
       "            0,    0,    0,    0,    0],\n",
       "        [ 101, 4931, 8937, 1012, 2054, 2015, 2039, 1029,  102, 2498, 2158, 1012,\n",
       "          102,    0,    0,    0,    0],\n",
       "        [ 101, 2054, 2003, 2293,  102, 2009, 2003, 1037, 2299, 2680, 2011, 1996,\n",
       "         3063, 2018, 2850, 4576,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb066d-75a3-4eb9-a614-d58b95134f10",
   "metadata": {},
   "source": [
    "## Context Encoder\n",
    "A context encoder is a component within a language model that processes tokenized input to generate meaningful representations (embeddings) of the text. These embeddings capture the semantic and syntactic relationships between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62123d4e-6b07-4f3a-a181-7fdae28308f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70c9c0b-7a07-494e-804c-ffc98cca94f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "context_encoder = DPRContextEncoder.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "631c1271-ed05-45c8-a520-29a7426aedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = context_encoder(**tokens_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3277a46-fbdf-404e-be7e-156243a77ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPRContextEncoderOutput(pooler_output=tensor([[ 0.2181,  0.5647, -0.2786,  ..., -0.3639,  0.7693, -0.0286],\n",
       "        [ 0.4470,  0.3929,  0.2845,  ..., -0.1393,  0.6474,  0.2293],\n",
       "        [ 0.1112,  0.0598,  0.1836,  ..., -0.4205, -0.0009,  0.3082]],\n",
       "       grad_fn=<SliceBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a019113-91e7-4c35-9d91-d665fbed6ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927303c-75f4-4475-b464-6f29c8786ab6",
   "metadata": {},
   "source": [
    "### In summary, while a context tokenizer handles text preprocessing by breaking it into tokens, a context encoder processes these tokens to produce meaningful representations that capture their relationships and context within the input text. Both are essential but serve distinctly different roles in NLP pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
